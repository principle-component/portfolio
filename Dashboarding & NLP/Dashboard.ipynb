{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 6,
        "hidden": true,
        "row": 56,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "source": [
    "Please note that some packages (and jupyter notebook specifically) require specific installed versions.\n",
    "\n",
    "To install jupyter dashboards and enable interactive viz, use the following commands\n",
    "\n",
    "```\n",
    "conda install notebook==5.5\n",
    "conda install -c conda-forge jupyter_dashboards\n",
    "jupyter nbextension enable jupyter_dashboards --py --sys-prefix\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": true,
        "row": 29,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from IPython.display import Javascript\n",
    "from plotnine import ggplot, aes, geom_line, scale_x_date, ylab, theme, geom_col, coord_flip, scale_x_discrete, geom_bar\n",
    "from plotnine import labs, scale_fill_gradient, scale_fill_manual, element_text\n",
    "\n",
    "#packages for LDA\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "import smart_open\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "\n",
    "#packages for log reg\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": true,
        "row": 43,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "comments = pd.read_csv('all_comments_withSentimentTickers_newestdata.csv')\n",
    "posts = pd.read_csv('all_posts_withSentimentTickers_newestdata.csv')\n",
    "prices = pd.read_csv('sp500_prices.csv')\n",
    "info = pd.read_csv('sp500_tickers.csv')\n",
    "\n",
    "prices['Ticker_Date'] = pd.to_datetime(prices['Ticker_Date'])\n",
    "prices['date'] = prices['Ticker_Date'].dt.date\n",
    "\n",
    "posts['created_utc'] = pd.to_datetime(posts['created_utc'],unit='s', origin='unix')\n",
    "posts['date'] = posts['created_utc'].dt.date\n",
    "\n",
    "comments['created_utc'] = pd.to_datetime(comments['created_utc'],unit='s', origin='unix')\n",
    "comments['date'] = comments['created_utc'].dt.date\n",
    "comments['body'] = comments['body'].astype(str)\n",
    "\n",
    "# Filter down dataset and expand on each mentioned ticker\n",
    "ticker_comments = comments[comments['tickers_mentioned'].notnull()].copy()\n",
    "ticker_comments['ticker_list'] = [x.replace(\" \",\"\").split(\",\") for x in ticker_comments['tickers_mentioned']]\n",
    "\n",
    "ticker_comments = ticker_comments.explode('ticker_list')\n",
    "ticker_comments = ticker_comments[ticker_comments['ticker_list'] != 'A'].copy() #filtering out bad data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 3,
        "height": 6,
        "hidden": false,
        "row": 0,
        "width": 6
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "#perform LDA\n",
    "\n",
    "addsw = 'going', 'would', 'go', 'think', 'money', 'dont', 'let', 'could', 'never', 'oh', 'oooh', 'got', 'like', 'youre', 'well', 'la', 'im', 'ive', 'whats', 'theyve', 'ohohh', 'youve', 'cant', 'wanna', 'another',  'theres', 'know', 'one', 'want', 'good', 'get','ill', 'market', 'time', 'stocks', 'people', 'buy', 'stock', 'years', 'inflation', 'trading','make'\n",
    "\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "for word in addsw: \n",
    "    stopwords.append(word)\n",
    "    \n",
    "def split_comment(comment):\n",
    "    x = [word for word in comment.split() if word not in stopwords]\n",
    "    return x\n",
    "\n",
    "def clean_comments(comment):\n",
    "    comment = str(comment)\n",
    "    comment = comment.lower()\n",
    "    comment = re.sub(r'[\\(\\[].*?[\\)\\]]', '', comment)\n",
    "    comment = re.sub('[%s]' % re.escape(string.punctuation), '', comment)\n",
    "    comment = os.linesep.join([s for s in comment.splitlines() if s])\n",
    "    return comment\n",
    "\n",
    "#comments['words'] = comments['body'].apply(lambda x: split_comment(x))\n",
    "\n",
    "#filter a dataset for last 3 days\n",
    "mx_dt = comments['created_utc'].max().date()\n",
    "recent_comments = comments[comments['created_utc'].dt.date >= (mx_dt - timedelta(days=3))].copy()\n",
    "recent_comments['body'] = recent_comments['body'].apply(lambda x: clean_comments(x))\n",
    "recent_comments['words'] = recent_comments['body'].apply(lambda x: split_comment(x))\n",
    "combined_comments = recent_comments.groupby('subreddit_id')['body'].apply(','.join).reset_index()\n",
    "cv = CountVectorizer(stop_words=stopwords, ngram_range=(1,2), analyzer='word')\n",
    "data_cv = cv.fit_transform(combined_comments.body)\n",
    "data_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n",
    "data_stop.index = combined_comments.index\n",
    "tds = data_stop.transpose()\n",
    "smart_open.open = smart_open.smart_open\n",
    "\n",
    "sparse_counts = scipy.sparse.csr_matrix(tds)\n",
    "corpus = matutils.Sparse2Corpus(sparse_counts)\n",
    "id2word = dict((v, k) for k, v in cv.vocabulary_.items())\n",
    "\n",
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\n",
    "print(\"LDA: Relevant topics for last 3 days\")\n",
    "#lda.print_topics()\n",
    "x = lda.print_topics()\n",
    "\n",
    "d = {}\n",
    "for i in range(len(x)):\n",
    "    lda0 = x[i][1]\n",
    "    sample = lda0.split('\"')\n",
    "    lst = []\n",
    "    for j in range(len(sample)):\n",
    "        if j % 2 != 0:\n",
    "            lst.append(sample[j])\n",
    "    df_name = 'df_{}'.format(i)\n",
    "    d[df_name] = pd.DataFrame(lst)\n",
    "\n",
    "df = pd.concat([d['df_0'], d['df_1']], axis=1)\n",
    "df = df.transpose().reset_index().drop([\"index\"], axis=1)\n",
    "df = df.rename(index={0: 'Topic 1', 1: 'Topic 2'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 6,
        "width": 6
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define our interactive GUI elements\n",
    "tickers = ticker_comments.groupby('ticker_list').count()['id'].sort_values(ascending=False)\n",
    "tickers = list(set(tickers[:20].index))\n",
    "tickers.sort()\n",
    "\n",
    "ticker_dropdown = widgets.Dropdown(\n",
    "    options=tickers,\n",
    "    value=tickers[0],\n",
    "    description='Stock Ticker:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "start_date = min(comments['date'])\n",
    "end_date = max(comments['date'])\n",
    "date_range = pd.date_range(start_date, end_date, freq='D')\n",
    "\n",
    "date_slider = widgets.SelectionRangeSlider(\n",
    "    options=[(date.strftime('%m/%d/%y'), date) for date in date_range],\n",
    "    index = (0, len(date_range)-1),\n",
    "    orientation='horizontal',\n",
    "    layout={'width': '500px'}\n",
    ")\n",
    "# Hook up interactivity to output cells\n",
    "def ticker_filter(ticker=ticker_dropdown.value, date_range = date_slider.value):\n",
    "    display(Javascript(\"Jupyter.notebook.execute_cells([5])\"))\n",
    "    display(Javascript(\"Jupyter.notebook.execute_cells([6])\"))\n",
    "    display(Javascript(\"Jupyter.notebook.execute_cells([7])\"))\n",
    "    display(Javascript(\"Jupyter.notebook.execute_cells([8])\"))\n",
    "    display(Javascript(\"Jupyter.notebook.execute_cells([9])\"))\n",
    "    display(Javascript(\"Jupyter.notebook.execute_cells([10])\"))\n",
    "interactive(ticker_filter, ticker=ticker_dropdown, date_range = date_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Define functions to generate dashboard elements\n",
    "def show_ticker_info(ticker):\n",
    "    ticker_info = info.loc[info['Symbol'] == ticker]\n",
    "    display(ticker_info.style.hide_index())\n",
    "\n",
    "def filter_data(ticker, start_date, end_date, full_df):\n",
    "    return full_df[(full_df['ticker_list'] == ticker)\n",
    "                  &(full_df['date'] >= pd.to_datetime(start_date))\n",
    "                  &(full_df['date'] <= pd.to_datetime(end_date))].copy()\n",
    "    \n",
    "def ticker_lines_new(ticker_df,start_date=date_slider.value[0], end_date=date_slider.value[1]):\n",
    "    ticker_df = ticker_df.groupby('date').agg(total_weighted_sentiment=('weighted_sentiment', 'mean')).reset_index()\n",
    "    ticker_values = prices.loc[prices['Symbol'] == ticker]\n",
    "    df = ticker_df.merge(ticker_values, on='date')\n",
    "    \n",
    "    fig,ax = plt.subplots(dpi=100)\n",
    "    ax.plot(df['date'],df['total_weighted_sentiment'],color='k',label='weighted sentiment')\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(df['date'],df['log_return'],color='g',label='log return')\n",
    "    for tick in ax.get_xticklabels():\n",
    "        tick.set_rotation(45)\n",
    "    plt.grid(True)\n",
    "    fig.legend(prop={'size': 7})\n",
    "    plt.title(ticker + ' Weighted Sentiment vs Log Return')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Weighted Sentiment Score')\n",
    "    ax2.set_ylabel('log return')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def ticker_mentions(start_date=date_slider.value[0], end_date=date_slider.value[1]):\n",
    "    df = ticker_comments.copy()\n",
    "    df = df[(df['date'] >= start_date)\n",
    "           &(df['date'] <= end_date)]\n",
    "    ticker_list = df['ticker_list'].value_counts().index.tolist()[::-1]\n",
    "    ticker_list = ticker_list[-30:]\n",
    "    ticker_cat = pd.Categorical(df['ticker_list'], categories=ticker_list)\n",
    "    df = df.assign(ticker_cat = ticker_cat)\n",
    "    df['is_active'] = [1 if x==ticker else 0 for x in df['ticker_list']]\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(6,5),dpi=100)\n",
    "    df.groupby('ticker_cat').count()['is_active'].plot.barh(color='g')\n",
    "    plt.ylabel('Ticker')\n",
    "    plt.xlabel('Count of Mentions')\n",
    "    plt.title('Top 30 Ticker Mentions over Date Range')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def subreddit_sentiment(ticker_df,start_date=date_slider.value[0], end_date=date_slider.value[1]):\n",
    "    fig,ax = plt.subplots(dpi=100)\n",
    "    ticker_df.groupby('subreddit').agg({'weighted_sentiment':'mean'}).plot.bar(color='g',legend=False,ax=ax)\n",
    "    plt.ylabel('Average Weighted Sentiment')\n",
    "    plt.title('Average Weighted Sentiment by Subreddit: Ticker ' + ticker)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def train_logistic_regression(data: pd.DataFrame, ticker: str):\n",
    "    \"\"\"Function to train logistic regression.\n",
    "    Outputs:\n",
    "        - clf: Trained model. Can be used for predictions\n",
    "        - report: Dictionary with model report (e.g. accuracy, precision, recall). Can be used to display model characteristics in dashboard\n",
    "        - cmd: Matplotlib figure with visual representation of confusion matrix. Can also be plotted in dashboard\n",
    "    \"\"\"\n",
    "    \n",
    "    ticker_data = data.copy()\n",
    "    # Filter by ticker\n",
    "    #ticker_data = data_local.loc[data_local['tickers_mentioned'] == ticker]\n",
    "    prices_local = prices.loc[prices['Symbol'] == ticker].copy()\n",
    "    \n",
    "    # Group by day (since we are predicting next day)\n",
    "    X = data.groupby(['created_utc']).agg(\n",
    "        #upvote_ratio=('upvote_ratio', 'mean'), \n",
    "        total_awards_received=('total_awards_received', 'mean'),\n",
    "        weighted_sentiment=('weighted_sentiment', 'mean'),\n",
    "        n_rows=('weighted_sentiment', 'count')\n",
    "        ).reset_index()\n",
    "    \n",
    "    # Add next day column so we can use that to join with prices data\n",
    "    X['next_day'] = pd.to_datetime(X.created_utc + pd.Timedelta(days=1)).dt.date\n",
    "    \n",
    "    # Add column to prices indicate whether price increased on a given date or not (1, 0)\n",
    "    prices_local['increased_next_day'] = (prices_local['return'] >= 0).astype(int)\n",
    "    #prices_local['Ticker_Date'] = pd.to_datetime(prices_local['Ticker_Date'])\n",
    "    \n",
    "    # Join the two datasets to get X & y in single df\n",
    "    data = X.merge(prices_local, left_on=['next_day'],\n",
    "                   right_on=['date'])[[#'tickers_mentioned',\n",
    "                                              'created_utc', 'weighted_sentiment',\n",
    "                                                        #'upvote_ratio',\n",
    "                                                        'total_awards_received', 'n_rows', 'increased_next_day']]\n",
    "    \n",
    "    \n",
    "    # Split into features and label\n",
    "    ticker_X = data[['weighted_sentiment', \n",
    "                                   #'upvote_ratio',\n",
    "                                   'total_awards_received', 'n_rows']]\n",
    "    ticker_y = data['increased_next_day']\n",
    "    # Train model\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(ticker_X, ticker_y)\n",
    "    # Predict\n",
    "    y_pred = clf.predict(ticker_X)\n",
    "    # Get scores\n",
    "    conf_m = confusion_matrix(ticker_y, y_pred, normalize='all')\n",
    "    report = classification_report(ticker_y, y_pred, output_dict=True)\n",
    "\n",
    "    # Make confusion matrix plot\n",
    "    #ConfusionMatrixDisplay(conf_m)\n",
    "    print(\"Binary Next Day Increase Logistic Regression Model\")\n",
    "    print(\"Next Day Prediction: \", clf.predict(ticker_X.iloc[-1:]))\n",
    "    return pd.DataFrame(report)#, cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 6,
        "height": 2,
        "hidden": false,
        "row": 6,
        "width": 4
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "ticker = ticker_dropdown.value\n",
    "\n",
    "ticker_dataframe = filter_data(ticker,start_date,end_date,ticker_comments)\n",
    "\n",
    "show_ticker_info(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 5,
        "height": 10,
        "hidden": false,
        "row": 10,
        "width": 6
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "start_date=date_slider.value[0]\n",
    "end_date=date_slider.value[1]\n",
    "ticker_lines_new(ticker_dataframe, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 13,
        "hidden": false,
        "row": 10,
        "width": 5
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "ticker_mentions(start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 7,
        "hidden": false,
        "row": 23,
        "width": 6
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "subreddit_sentiment(ticker_dataframe,start_date,end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 6,
        "height": 8,
        "hidden": false,
        "row": 23,
        "width": 5
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "train_logistic_regression(ticker_dataframe,ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 2,
        "hidden": false,
        "row": 35,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 0,
        "height": 4,
        "hidden": false,
        "row": 31,
        "width": 12
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "hidden": true
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "extensions": {
     "jupyter_dashboards": {
      "version": 1,
      "views": {
       "grid_default": {
        "col": 5,
        "height": 3,
        "hidden": false,
        "row": 20,
        "width": 6
       },
       "report_default": {}
      }
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "grid_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b3e6bdbbf5153677d0a3bfedb94ab49d1202a0594879e5caf34a7c8e487087c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
